
Write and run a computer program to estimate $\mathbb{E}_\pi[(X_1-X_2)/(1+X_3+X_4X_5)]$ by using a rejection sampler with the above $f$. Discuss the extent to which this algorithm works well.\\

First let $(x1,x2,x3,x4,x5)$ be denoted as $x$. In order to use the rejection sampler, we have to find a constant $K$ such that $g(x)\le Kf(x)$, so we need to find the maximum value of $g$ in the box $(0,2)^5$. The maximum value is found by using \textsf{optim} in R:
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{goptim} \hlkwb{=} \hlkwa{function}\hlstd{(}\hlkwc{x}\hlstd{) \{}
  \hlstd{x1}\hlkwb{=}\hlstd{x[}\hlnum{1}\hlstd{]}
  \hlstd{x2}\hlkwb{=}\hlstd{x[}\hlnum{2}\hlstd{]}
  \hlstd{x3}\hlkwb{=}\hlstd{x[}\hlnum{3}\hlstd{]}
  \hlstd{x4}\hlkwb{=}\hlstd{x[}\hlnum{4}\hlstd{]}
  \hlstd{x5}\hlkwb{=}\hlstd{x[}\hlnum{5}\hlstd{]}
  \hlopt{-}\hlstd{(x1}\hlopt{+}\hlstd{A}\hlopt{+}\hlnum{2}\hlstd{)}\hlopt{^}\hlstd{(x2}\hlopt{+}\hlnum{3}\hlstd{)}\hlopt{*}\hlstd{(}\hlnum{1}\hlopt{+}\hlkwd{cos}\hlstd{((B}\hlopt{+}\hlnum{3}\hlstd{)}\hlopt{*}\hlstd{x3))}\hlopt{*}\hlstd{(}\hlkwd{exp}\hlstd{((}\hlnum{12}\hlopt{-}\hlstd{C)}\hlopt{*}\hlstd{x4))}\hlopt{*}\hlkwd{abs}\hlstd{(x4}\hlopt{-}\hlnum{3}\hlopt{*}\hlstd{x5)}\hlopt{^}\hlstd{(D}\hlopt{+}\hlnum{2}\hlstd{)}
\hlstd{\}}
\hlkwd{optim}\hlstd{(}\hlkwd{rep}\hlstd{(}\hlnum{1}\hlstd{,}\hlnum{5}\hlstd{),goptim,}\hlkwa{NULL}\hlstd{,}\hlkwc{method}\hlstd{=}\hlstr{"L-BFGS-B"}\hlstd{,}\hlkwc{lower}\hlstd{=}\hlkwd{rep}\hlstd{(}\hlnum{0}\hlstd{,}\hlnum{5}\hlstd{),}\hlkwc{upper}\hlstd{=}\hlkwd{rep}\hlstd{(}\hlnum{2}\hlstd{,}\hlnum{5}\hlstd{))}
\end{alltt}
\end{kframe}
\end{knitrout}
Notice that \textsf{optim} finds the minimum instead of maximum value, so we have to define \textsf{goptim} as the negative of function $g(x)$.
The result of running the code above gives:
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{verbatim}
## $par
## [1] 2 2 0 2 2
## 
## $value
## [1] -5.368181e+13
## 
## $counts
## function gradient 
##        2        2 
## 
## $convergence
## [1] 0
## 
## $message
## [1] "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL"
\end{verbatim}
\end{kframe}
\end{knitrout}
Since $f(x)=\frac{1}{32}$ is constant in $(0,2)^5$, we need to set $K\ge 32*\max(g)=$1.718e+15, and since $K$ does not need to be tight bound, we pick $K=$2e+15 for easier computation. Now we can proceed to estimate the expectation $\mathbb{E}_\pi[(X_1-X_2)/(1+X_3+X_4X_5)]$ by using a rejection sampler with $f$. Note that we adjust the number of samples taken from $f$ to 1e+7 this time because the low acceptance rate. We have to increase the number of samples in order to have enough samples for the $\pi$ distribution.\\
Source code:
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlcom{#Estimate E_pi((X1-X2)/(1+X3+X4X5)) where pi=cg using rejection sampling techniques }
\hlcom{# with density f = uniform(0,2)^5}

\hlstd{A} \hlkwb{=} \hlnum{7}\hlstd{; B} \hlkwb{=} \hlnum{6}\hlstd{; C} \hlkwb{=} \hlnum{6}\hlstd{; D} \hlkwb{=} \hlnum{3}

\hlstd{fn} \hlkwb{=} \hlkwa{function}\hlstd{(}\hlkwc{x1}\hlstd{,}\hlkwc{x2}\hlstd{,}\hlkwc{x3}\hlstd{,}\hlkwc{x4}\hlstd{,}\hlkwc{x5}\hlstd{) \{(x1}\hlopt{-}\hlstd{x2)}\hlopt{/}\hlstd{(}\hlnum{1}\hlopt{+}\hlstd{x3}\hlopt{+}\hlstd{x4}\hlopt{*}\hlstd{x5)\}}
\hlstd{g} \hlkwb{=} \hlkwa{function}\hlstd{(}\hlkwc{x1}\hlstd{,}\hlkwc{x2}\hlstd{,}\hlkwc{x3}\hlstd{,}\hlkwc{x4}\hlstd{,}\hlkwc{x5}\hlstd{)}
\hlstd{\{(x1}\hlopt{+}\hlstd{A}\hlopt{+}\hlnum{2}\hlstd{)}\hlopt{^}\hlstd{(x2}\hlopt{+}\hlnum{3}\hlstd{)}\hlopt{*}\hlstd{(}\hlnum{1}\hlopt{+}\hlkwd{cos}\hlstd{((B}\hlopt{+}\hlnum{3}\hlstd{)}\hlopt{*}\hlstd{x3))}\hlopt{*}\hlstd{(}\hlkwd{exp}\hlstd{((}\hlnum{12}\hlopt{-}\hlstd{C)}\hlopt{*}\hlstd{x4))}\hlopt{*}\hlkwd{abs}\hlstd{(x4}\hlopt{-}\hlnum{3}\hlopt{*}\hlstd{x5)}\hlopt{^}\hlstd{(D}\hlopt{+}\hlnum{2}\hlstd{)\}}
\hlstd{K} \hlkwb{=} \hlnum{2e+15}

\hlstd{M} \hlkwb{=} \hlnum{1e+7}

\hlcom{#sample from f}
\hlstd{x1} \hlkwb{=} \hlkwd{runif}\hlstd{(M,}\hlnum{0}\hlstd{,}\hlnum{2}\hlstd{)}
\hlstd{x2} \hlkwb{=} \hlkwd{runif}\hlstd{(M,}\hlnum{0}\hlstd{,}\hlnum{2}\hlstd{)}
\hlstd{x3} \hlkwb{=} \hlkwd{runif}\hlstd{(M,}\hlnum{0}\hlstd{,}\hlnum{2}\hlstd{)}
\hlstd{x4} \hlkwb{=} \hlkwd{runif}\hlstd{(M,}\hlnum{0}\hlstd{,}\hlnum{2}\hlstd{)}
\hlstd{x5} \hlkwb{=} \hlkwd{runif}\hlstd{(M,}\hlnum{0}\hlstd{,}\hlnum{2}\hlstd{)}

\hlcom{#generate r.v. for acceptance}
\hlstd{u} \hlkwb{=} \hlkwd{runif}\hlstd{(M)}
\hlstd{acc} \hlkwb{=} \hlstd{(u}\hlopt{<=}\hlnum{32}\hlopt{*}\hlkwd{g}\hlstd{(x1,x2,x3,x4,x5)}\hlopt{/}\hlstd{K)}

\hlstd{fnlist} \hlkwb{=} \hlkwd{fn}\hlstd{(x1[acc],x2[acc],x3[acc],x4[acc],x5[acc])}
\hlstd{funcmean} \hlkwb{=} \hlkwd{mean}\hlstd{(fnlist)}
\hlstd{funcse} \hlkwb{=} \hlkwd{sd}\hlstd{(funclist)}\hlopt{/}\hlkwd{sqrt}\hlstd{(}\hlkwd{sum}\hlstd{(acc))}
\hlstd{accrate} \hlkwb{=} \hlkwd{sum}\hlstd{(acc)}\hlopt{/}\hlstd{M}

\hlkwd{cat}\hlstd{(}\hlstr{'M = '}\hlstd{,M,}\hlstr{'\textbackslash{}n'}\hlstd{)}
\hlkwd{cat}\hlstd{(}\hlstr{'Number of samples accepted = '}\hlstd{,}\hlkwd{sum}\hlstd{(acc),}\hlstr{', acceptance rate = '}\hlstd{,accrate,}\hlstr{'\textbackslash{}n'}\hlstd{)}
\hlkwd{cat}\hlstd{(}\hlstr{'Estimate = '}\hlstd{,funcmean,}\hlstr{'\textbackslash{}n'}\hlstd{)}
\hlkwd{cat}\hlstd{(}\hlstr{'Standard error = '}\hlstd{,funcse,}\hlstr{'\textbackslash{}n'}\hlstd{)}
\end{alltt}
\end{kframe}
\end{knitrout}
Output of several runs:
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{verbatim}
## M =  1e+07 
## Number of samples accepted =  6833 , acceptance rate =  0.0006833 
## Estimate =  -0.08753893 
## Standard error =  0.005837405
## M =  1e+07 
## Number of samples accepted =  6794 , acceptance rate =  0.0006794 
## Estimate =  -0.088423 
## Standard error =  0.005854135
## M =  1e+07 
## Number of samples accepted =  6885 , acceptance rate =  0.0006885 
## Estimate =  -0.08980898 
## Standard error =  0.005815319
## M =  1e+07 
## Number of samples accepted =  6915 , acceptance rate =  0.0006915 
## Estimate =  -0.08679519 
## Standard error =  0.005802691
## M =  1e+07 
## Number of samples accepted =  6973 , acceptance rate =  0.0006973 
## Estimate =  -0.09099398 
## Standard error =  0.005778508
\end{verbatim}
\end{kframe}
\end{knitrout}
The tabulated result is as follows:\\
\begin{center}
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}
\begin{tabular}{r|r}
\hline
estimation & standard\_error\\
\hline
-0.0875389 & 0.0058374\\
\hline
-0.0884230 & 0.0058541\\
\hline
-0.0898090 & 0.0058153\\
\hline
-0.0867952 & 0.0058027\\
\hline
-0.0909940 & 0.0057785\\
\hline
\end{tabular}


\end{knitrout}
\end{center}
The estimated value for $\mathbb{E}_\pi[(X_1-X_2)/(1+X_3+X_4X_5)]$ is \ensuremath{-0.089} with standard error of approximately 0.00582. The results from the five runs are quite consistent.\\
However, since the acceptance rate is so low (about \ensuremath{6.88\times 10^{-4}}), the algorithm is not as effective as the importance sampler in Problem 4. The computation time is a lot longer and standard error of the result is also larger than the importance sampler in Problem 4. 
