<<echo=FALSE>>=
knitr::read_chunk('Rcode/hwp8b.R')
@
(b) With a Langevin (Metropolis-Hastings) algorithm with proposals $Y_n$~$N(X_{n-1}+\frac{1}{2}\sigma^2g'(X_{n-1})/g(X_{n-1}),\sigma^2)$.
First we calculate analycically $g'(x)$:
\begin{align*}
\frac{dg}{dx} &= 
\begin{cases}
  \frac{d}{dx}\left(e^{-x/10}(1+\cos(x)\sin(x^3))\right) &\text{for }x>0\\
  \frac{d}{dx}\left(e^{x/10}(1+\cos(x)\sin(x^3))\right) &\text{for }x<0
\end{cases}\\
  &=\begin{cases}
  -\frac{1}{10}e^{-x/10}(1+\cos(x)\sin(x^3)) +e^{-x/10}(3x^2\cos(x)\cos(x^3)-\sin(x)\sin(x^3)) &\text{for }x>0\\
  \frac{1}{10}e^{x/10}(1+\cos(x)\sin(x^3)) +e^{x/10}(3x^2\cos(x)\cos(x^3)-\sin(x)\sin(x^3)) &\text{for }x<0\\
  \end{cases}
\end{align*}
With the closed form of $g'(x)$ calculated, we can implement the Langevin algorithm with the following code
<<hwp8b,eval=FALSE>>=
@
Output of several runs:
<<echo=FALSE>>=
acc_rate <-numeric(5)
estimation <- numeric(5)
seiid <- numeric(5)
varfact_r <- numeric(5)
standard_error <- numeric(5)
source('Rcode/hwp8b.R')
estimation[1] <- funcmean
standard_error[1] <- funcse
acc_rate[1] <- accrate
seiid[1] <- funciidse
varfact_r[1] <- varfact
source('Rcode/hwp8b.R')
estimation[2] <- funcmean
standard_error[2] <- funcse
acc_rate[2] <- accrate
seiid[2] <- funciidse
varfact_r[2] <- varfact
source('Rcode/hwp8b.R')
estimation[3] <- funcmean
standard_error[3] <- funcse
acc_rate[3] <- accrate
seiid[3] <- funciidse
varfact_r[3] <- varfact
source('Rcode/hwp8b.R')
estimation[4] <- funcmean
standard_error[4] <- funcse
acc_rate[4] <- accrate
seiid[4] <- funciidse
varfact_r[4] <- varfact
source('Rcode/hwp8b.R')
estimation[5] <- funcmean
standard_error[5] <- funcse
acc_rate[5] <- accrate
seiid[5] <- funciidse
varfact_r[5] <- varfact
meanse <- data.frame(estimation,acc_rate,seiid,varfact_r,standard_error)
@
The tabulated result is as follows:\\
\begin{center}
<<echo=FALSE>>=
knitr::kable(meanse)
@
\end{center}
We notice that the varfact values are quite small, indicating that the samples are not strongly correlated. To verify the effectiveness of the sampler, here we plot the value of $x$ after the burn-in stage
\begin{figure}[H]
  \centering
<<p8bplot, dev.args=list(pointsize=8),fig.width=6, fig.height=5>>=
xlist8b = xlist
plot(xlist8b[(B+1):(B+M)],type='l')
@
		\caption{$x$ values of Langevin algorithm after burning-in}
\end{figure}

Notice that the estimation is by no means consistent, and the varfact values are huge, making the standar error large too.\\
\\
(c) Compare the two algorithms and discuss which one is "better".\\
In this case, the random-walk Metropolis algorithm clearly outperforms the Langevin algorithm as it produces more reasonable estimates to the integral. The Langevin algorithm is designed to work well for well behaved functions. In particular, if the function $g'(x)/g(x)$ is smooth, then the Langevin algorithm is going to outperform RWM algorithm. However, if we plot $g'(x)/g(x)$, we notice that it varies significantly even on a log-scaled plot
\begin{figure}[H]
  \centering
 <<p8cplot, dev.args=list(pointsize=8),fig.width=6,fig.height=5>>=
xline = seq(-10,10,0.01)
plot(log(gp(xline)/g(xline)),type='l')
@
\end{figure}
Since to the function $g'(x)/g(x)$ is unstable, a discrete approximation of this function will fail. As a result, the RWM algorithm outperforms the Langevin algorithm significantly.