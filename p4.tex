
Write and run a computer program to estimate $\mathbb{E}_\pi[(X_1-X_2)/(1+X_3+X_4X_5)]$by importance sampler with the above $f$. Discuss the extent to which this algorithm works well.\\
First write the expectation under the original distribution in terms of an expectation under $f$, and write $x=(x_1,x_2,x_3,x_4,x_5)$ below
\begin{align*}
\mathbb{E}_\pi\left[\frac{X_1-X_2}{1+X_3+X_4X_5}\right]&=\int \frac{x_1-x_2}{1+x_3+x_4x_5} \pi(x)\,dx\\
  &=\int \frac{\frac{x_1-x_2}{1+x_3+x_4x_5} \frac{g(x)}{f(x)}f(x)\,dx}{\frac{g(x)}{f(x)}f(x)\,dx}\\
  &=\frac{\mathbb{E}_f[\frac{x_1-x_2}{1+x_3+x_4x_5} \frac{g(x)}{f(x)}]}{\mathbb{E}_f[\frac{g(x)}{f(x)}]}\\
  &\approx\frac{\frac{1}{M}\sum_{i=1}^M\left(\frac{x_1^{(i)}-x_2^{(i)}}{1+x_3^{(i)}+x_4^{(i)}x_5^{(i)}} \frac{g(x^{(i)})}{f(x^{(i)})}\right)}{\frac{1}{M}\sum_{i=1}^M\left(\frac{g(x^{(i)})}{f(x^{(i)})}\right)}\\
  &=\frac{\sum_{i=1}^M\left(\frac{x_1^{(i)}-x_2^{(i)}}{1+x_3^{(i)}+x_4^{(i)}x_5^{(i)}} \frac{g(x^{(i)})}{f(x^{(i)})}\right)}{\sum_{i=1}^M\left(\frac{g(x^{(i)})}{f(x^{(i)})}\right)}
\end{align*}
Since for all points sampled under $f$, $f(x_1,x_2,x_3,x_4,x_5)=\frac{1}{32}$, 
\begin{align*}
\mathbb{E}_\pi\left[\frac{X_1-X_2}{1+X_3+X_4X_5}\right]=\frac{\sum_{i=1}^M\left(\frac{x_1^{(i)}-x_2^{(i)}}{1+x_3^{(i)}+x_4^{(i)}x_5^{(i)}} 32g(x^{(i)})\right)}{\sum_{i=1}^M\left(32g(x^{(i)})\right)}=\frac{\sum_{i=1}^M\left(\frac{x_1^{(i)}-x_2^{(i)}}{1+x_3^{(i)}+x_4^{(i)}x_5^{(i)}} g(x^{(i)})\right)}{\sum_{i=1}^M\left(g(x^{(i)})\right)}
\end{align*}
Notice that the importance sampling algorithm reduces to evaluating the expectation with integration, since $f$ is constant. \\
Also, because we are using importance sampling, we can not get an estimated of the standard error from a single run. So we estimate the standard error using results from multiple (10) runs.\\

Source code:
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlcom{#Estimate E_pi((X1-X2)/(1+X3+X4X5)) where pi=cg using importance sampling techniques }
\hlcom{# with density f = uniform(0,2)^5}

\hlstd{A} \hlkwb{=} \hlnum{7}\hlstd{; B} \hlkwb{=} \hlnum{6}\hlstd{; C} \hlkwb{=} \hlnum{6}\hlstd{; D} \hlkwb{=} \hlnum{3}

\hlstd{fn} \hlkwb{=} \hlkwa{function}\hlstd{(}\hlkwc{x1}\hlstd{,}\hlkwc{x2}\hlstd{,}\hlkwc{x3}\hlstd{,}\hlkwc{x4}\hlstd{,}\hlkwc{x5}\hlstd{) \{(x1}\hlopt{-}\hlstd{x2)}\hlopt{/}\hlstd{(}\hlnum{1}\hlopt{+}\hlstd{x3}\hlopt{+}\hlstd{x4}\hlopt{*}\hlstd{x5)\}}
\hlstd{g} \hlkwb{=} \hlkwa{function}\hlstd{(}\hlkwc{x1}\hlstd{,}\hlkwc{x2}\hlstd{,}\hlkwc{x3}\hlstd{,}\hlkwc{x4}\hlstd{,}\hlkwc{x5}\hlstd{)}
  \hlstd{\{(x1}\hlopt{+}\hlstd{A}\hlopt{+}\hlnum{2}\hlstd{)}\hlopt{^}\hlstd{(x2}\hlopt{+}\hlnum{3}\hlstd{)}\hlopt{*}\hlstd{(}\hlnum{1}\hlopt{+}\hlkwd{cos}\hlstd{((B}\hlopt{+}\hlnum{3}\hlstd{)}\hlopt{*}\hlstd{x3))}\hlopt{*}\hlstd{(}\hlkwd{exp}\hlstd{((}\hlnum{12}\hlopt{-}\hlstd{C)}\hlopt{*}\hlstd{x4))}\hlopt{*}\hlkwd{abs}\hlstd{(x4}\hlopt{-}\hlnum{3}\hlopt{*}\hlstd{x5)}\hlopt{^}\hlstd{(D}\hlopt{+}\hlnum{2}\hlstd{)\}}

\hlcom{#number of runs}
\hlstd{R}\hlkwb{=}\hlnum{10}
\hlcom{#number of cases}
\hlstd{M}\hlkwb{=}\hlnum{10}\hlopt{^}\hlnum{6}

\hlstd{estlist}\hlkwb{=}\hlkwd{numeric}\hlstd{(R)}

\hlkwd{cat}\hlstd{(}\hlstr{'M = '}\hlstd{,M,}\hlstr{'\textbackslash{}n'}\hlstd{)}
\hlkwa{for} \hlstd{(i} \hlkwa{in} \hlnum{1}\hlopt{:}\hlstd{R) \{}
  \hlcom{#sample from unif(0,2)^5}
  \hlstd{x1} \hlkwb{=} \hlkwd{runif}\hlstd{(M,}\hlnum{0}\hlstd{,}\hlnum{2}\hlstd{)}
  \hlstd{x2} \hlkwb{=} \hlkwd{runif}\hlstd{(M,}\hlnum{0}\hlstd{,}\hlnum{2}\hlstd{)}
  \hlstd{x3} \hlkwb{=} \hlkwd{runif}\hlstd{(M,}\hlnum{0}\hlstd{,}\hlnum{2}\hlstd{)}
  \hlstd{x4} \hlkwb{=} \hlkwd{runif}\hlstd{(M,}\hlnum{0}\hlstd{,}\hlnum{2}\hlstd{)}
  \hlstd{x5} \hlkwb{=} \hlkwd{runif}\hlstd{(M,}\hlnum{0}\hlstd{,}\hlnum{2}\hlstd{)}

  \hlstd{fnlist} \hlkwb{=} \hlkwd{fn}\hlstd{(x1,x2,x3,x4,x5)}
  \hlstd{glist} \hlkwb{=} \hlkwd{g}\hlstd{(x1,x2,x3,x4,x5)}
  \hlstd{estlist[i]} \hlkwb{=} \hlkwd{sum}\hlstd{(fnlist}\hlopt{*}\hlstd{glist)}\hlopt{/}\hlkwd{sum}\hlstd{(glist)}
  \hlkwd{cat}\hlstd{(}\hlstr{'run = '}\hlstd{,i,}\hlstr{', estimate = '}\hlstd{,estlist[i],}\hlstr{'\textbackslash{}n'}\hlstd{)}
\hlstd{\}}

\hlstd{estmean} \hlkwb{=} \hlkwd{mean}\hlstd{(estlist)}
\hlstd{estse} \hlkwb{=} \hlkwd{sd}\hlstd{(estlist)}

\hlkwd{cat}\hlstd{(}\hlstr{'mean estimate = '}\hlstd{,funcmean,}\hlstr{'\textbackslash{}n'}\hlstd{)}
\hlkwd{cat}\hlstd{(}\hlstr{'estimate standard error = '}\hlstd{,funcse,}\hlstr{'\textbackslash{}n'}\hlstd{)}
\end{alltt}
\end{kframe}
\end{knitrout}
Output of several runs:
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{verbatim}
## M =  1e+06 
## run =  1 , estimate =  -0.08714028 
## run =  2 , estimate =  -0.08637034 
## run =  3 , estimate =  -0.08834511 
## run =  4 , estimate =  -0.08894496 
## run =  5 , estimate =  -0.08718104 
## run =  6 , estimate =  -0.08830067 
## run =  7 , estimate =  -0.08921454 
## run =  8 , estimate =  -0.08562825 
## run =  9 , estimate =  -0.08842424 
## run =  10 , estimate =  -0.08817753 
## mean estimate =  -0.08777848 
## estimate standard error =  0.005801013
\end{verbatim}
\end{kframe}
\end{knitrout}
The tabulated result is as follows:\\
\begin{center}
\begin{tabular}{c c c c c }
\ensuremath{-0.0871403} & \ensuremath{-0.0863703} & \ensuremath{-0.0883451} & \ensuremath{-0.088945} & \ensuremath{-0.087181} \\
\ensuremath{-0.0883007} & \ensuremath{-0.0892145} & \ensuremath{-0.0856282} & \ensuremath{-0.0884242} & \ensuremath{-0.0881775} 
\end{tabular}
\end{center}
The mean of all estimates is \ensuremath{-0.0877727} with standard error 0.0011529.\\
The algorithm works reasonably well because using importance sampling, we can easily pick samples from a much easier distribution. The result seems to be strong as well as the estimates across different runs are fairly consistent and the standard error is small.
